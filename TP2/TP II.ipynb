{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP II.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Njfs_nQTLrCR","colab_type":"text"},"source":["## MOUNT DRIVE DATA\n"]},{"cell_type":"code","metadata":{"id":"Q9eiKoEPLmdj","colab_type":"code","outputId":"03810786-a68d-482e-aa28-9a04ff1afb42","executionInfo":{"status":"ok","timestamp":1574562261503,"user_tz":180,"elapsed":2013,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import sys\n","\n","drive.mount('/content/gdrive/')\n","sys.path.append('/content/gdrive/My Drive/python')"],"execution_count":130,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jbSx4OIZLnAD","colab_type":"code","outputId":"6fd4c246-6af7-4c39-c41f-ca147bd3c00d","executionInfo":{"status":"ok","timestamp":1574562407177,"user_tz":180,"elapsed":17265,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["%cd /content/gdrive/My\\ Drive/NLP\\ TP\\ II\n","!ls ./\n","!export CUDA_VISIBLE_DEVICES=\"0\""],"execution_count":146,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/NLP TP II\n"," acc_class.png\t       modelo1.pt\t skip_s100.txt\t   'TP II.ipynb'\n"," contagem_class.png    modelo2.pt\t skip_s100.txt.pt  'TP II NLP.ipynb'\n"," data.csv\t       model.pt\t\t skip_s50.txt\t    train_data.csv\n"," macmorpho-test.txt    perc_embedd.png\t skip_s50.txt.pt    tut1-model.pt\n"," macmorpho-train.txt   runs\t\t test_data.csv\n","\u001b[K     |████████████████████████████████| 81kB 2.5MB/s \n","\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.6.3 which is incompatible.\u001b[0m\n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E1lKv2f4NUyO","colab_type":"text"},"source":["## IMPORTS E DEFINICAO DOS HYPER-PARAMETROS"]},{"cell_type":"code","metadata":{"id":"DwarxY76MwAe","colab_type":"code","outputId":"d0146e16-d481-41f5-b1dc-50918d8931b9","executionInfo":{"status":"ok","timestamp":1574562293268,"user_tz":180,"elapsed":33741,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils import data\n","from torch.backends import cudnn\n","from torchtext.data import Field\n","from torchtext import data\n","from torchtext.vocab import Vectors\n","#from torch.utils.tensorboard import SummaryWriter\n","\n","import tensorflow as tf\n","from tensorflow import summary\n","import datetime, os\n","%load_ext tensorboard\n","\n","import matplotlib.pyplot as plt\n","\n","import time\n","import random\n","\n","from gensim.models import KeyedVectors\n","import pandas as pd\n","\n","cudnn.benchmark = True\n","\n","# Setting predefined arguments.\n","args = {\n","    'epoch_num': 80,     # 80 # Number of epochs.\n","    'lr': 3e-5,           # Learning rate.\n","    'weight_decay': 5e-4, # 5e-4 # L2 penalty.\n","    'momentum': 0.9,      # Momentum.\n","    'num_workers': 6,     # Number of workers on data loader.\n","    'batch_size': 64,     # Mini-batch size.\n","    'clip_norm': 6.0,     # 6  # Upper limit on gradient L2 norm ###\n","    'min_freq' : 2.0,\n","    'gradient_clipping' : True,\n","    'epoch_finetune' : 40, # 40\n","    'num_layers' : 2, # 2\n","    'seed' : 42,\n","    'hidden_dim' : 100,\n","    'embedding_dim': 100,\n","    'path_embed' : 'skip_s100.txt',\n","    'preprocess' : False,\n","    'train_model' : True\n","}\n","\n","if torch.cuda.is_available():\n","    args['device'] = torch.device('cuda')\n","else:\n","    args['device'] = torch.device('cpu')\n","\n","print(args['device'])"],"execution_count":132,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n","cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eG3h9mnTL1x1","colab_type":"text"},"source":["## PRÉ PROCESSAMENTO DOS DADOS"]},{"cell_type":"code","metadata":{"id":"lKopAoCVLoVY","colab_type":"code","colab":{}},"source":["def preprocess_data(path):\n","  sizes = []\n","  text = []\n","  labels = []\n","  with open(path, 'r') as raw_train:\n","\n","    for line in raw_train:\n","      phrase = \"\"\n","      phrase_labels = \"\"\n","\n","      words = line.split(' ')\n","      len_seq = len(words)\n","\n","      sizes.append(len(line.split()))\n","\n","      for index, word in enumerate(words):\n","        w = word.split('_')\n","        phrase_labels = phrase_labels + w[1].strip('\\n')\n","        phrase = phrase + w[0].lower()\n","\n","        if index != len_seq-1:\n","          phrase = phrase + ' '\n","          phrase_labels = phrase_labels + ' '\n","\n","      labels.append(phrase_labels)\n","      text.append(phrase)\n","\n","  return text, labels, sizes\n","\n","def save_data(output_file, text, labels):\n","  dataset = {'text': text, 'labels': labels}\n","  dados = pd.DataFrame(dataset)\n","  dados.to_csv(output_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OqqgJJGLpnF","colab_type":"code","colab":{}},"source":["if args['preprocess']:\n","  train_text, train_labels, train_size = preprocess_data('macmorpho-train.txt')\n","  test_text, test_labels, test_size = preprocess_data('macmorpho-test.txt')\n","  save_data('train_data.csv', train_text, train_labels)\n","  save_data('test_data.csv', test_text, test_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2sqPprNMH-K","colab_type":"text"},"source":["## CARREGANDO DADOS PARA TORCHTEXT"]},{"cell_type":"code","metadata":{"id":"Uemz6OhDL_DQ","colab_type":"code","outputId":"a4e7c6ba-a9d3-4ea0-b779-0f83aad76845","executionInfo":{"status":"ok","timestamp":1574562294758,"user_tz":180,"elapsed":35189,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["tokenize = lambda x: x.split()\n","TEXT = Field(tokenize=tokenize, lower=True, include_lengths=True)\n","\n","LABEL = Field(sequential=True, use_vocab=True, tokenize=tokenize)\n","\n","fields = [(None, None), ('text', TEXT), ('labels', LABEL)]\n","\n","train_data, test_data = data.TabularDataset.splits(\n","  path = '.',\n","  train = 'train_data.csv',\n","  test = 'test_data.csv',\n","  format = 'csv',\n","  fields = fields,\n","  skip_header = True)\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(args['seed']), split_ratio=0.9)\n","\n","print(vars(train_data.examples[0]))\n","\n","print(f'TRAIN DATA LENGTH: {len(train_data)} | VALIDATION DATA LENGTH: {len(valid_data)}')"],"execution_count":135,"outputs":[{"output_type":"stream","text":["{'text': ['segundo', 'o', 'diretor', 'do', 'departamento', 'geral', 'de', 'polícia', 'especializada', ',', 'luiz', 'mariano', ',', 'o', 'terceiro', 'sequestro', 'pode', 'ter', 'sido', 'tramado', 'por', 'ex-empregados', 'do', 'empresário', 'ou', 'por', 'pessoas', 'ligadas', 'à', 'família', '.'], 'labels': ['PREP', 'ART', 'N', 'PREP+ART', 'NPROP', 'NPROP', 'NPROP', 'NPROP', 'NPROP', 'PU', 'NPROP', 'NPROP', 'PU', 'ART', 'ADJ', 'N', 'V', 'V', 'PCP', 'PCP', 'PREP', 'N', 'PREP+ART', 'N', 'KC', 'PREP', 'N', 'PCP', 'PREP+ART', 'N', 'PU']}\n","TRAIN DATA LENGTH: 34153 | VALIDATION DATA LENGTH: 3795\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aVj_yFZ0N7yc","colab_type":"text"},"source":["### Carregando Embedding pré treinado"]},{"cell_type":"code","metadata":{"id":"BvXKeqcaMA-Y","colab_type":"code","colab":{}},"source":["vectors = Vectors(name=args['path_embed'], cache='.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GL2-Du1OOGtp","colab_type":"text"},"source":["### Construindo vocabulario"]},{"cell_type":"code","metadata":{"id":"7VDbcrtDOAfT","colab_type":"code","outputId":"10969733-64d1-4f56-c9ae-a846ec2decc5","executionInfo":{"status":"ok","timestamp":1574562297401,"user_tz":180,"elapsed":37809,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["TEXT.build_vocab(train_data, \n","                 min_freq = args['min_freq'],\n","                 vectors = vectors, \n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)\n","\n","print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"],"execution_count":137,"outputs":[{"output_type":"stream","text":["Unique tokens in TEXT vocabulary: 23245\n","Unique tokens in LABEL vocabulary: 28\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JnYR-pgLOKL6","colab_type":"code","outputId":"25823a88-5f4d-4664-fef6-cbe2052e258e","executionInfo":{"status":"ok","timestamp":1574562297404,"user_tz":180,"elapsed":37798,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":469}},"source":["def tag_percentage(tag_counts):\n","    \n","  total_count = sum([count for tag, count in tag_counts])\n","  \n","  tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n","      \n","  return tag_counts_percentages\n","\n","for tag, count, percent in tag_percentage(LABEL.vocab.freqs.most_common()):\n","  print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"],"execution_count":138,"outputs":[{"output_type":"stream","text":["N\t140430\t21.4%\n","PU\t95407\t14.6%\n","V\t68262\t10.4%\n","NPROP\t64649\t 9.9%\n","PREP\t63728\t 9.7%\n","ART\t47777\t 7.3%\n","PREP+ART\t41040\t 6.3%\n","ADJ\t29528\t 4.5%\n","ADV\t16582\t 2.5%\n","KC\t16090\t 2.5%\n","PCP\t13582\t 2.1%\n","NUM\t11649\t 1.8%\n","PROADJ\t10246\t 1.6%\n","KS\t8127\t 1.2%\n","PRO-KS\t7449\t 1.1%\n","PROPESS\t7393\t 1.1%\n","PROSUB\t4085\t 0.6%\n","PDEN\t3897\t 0.6%\n","CUR\t1826\t 0.3%\n","PREP+PROADJ\t1209\t 0.2%\n","ADV-KS\t708\t 0.1%\n","PREP+PROSUB\t459\t 0.1%\n","PREP+PROPESS\t345\t 0.1%\n","IN\t149\t 0.0%\n","PREP+PRO-KS\t138\t 0.0%\n","PREP+ADV\t46\t 0.0%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u8k2ZrGKOV_F","colab_type":"text"},"source":["### Construindo Bucket Iterator"]},{"cell_type":"code","metadata":{"id":"t8jOTsy9OTvP","colab_type":"code","colab":{}},"source":["train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    sort_key = lambda x:len(x.text),\n","    sort_within_batch = True,\n","    batch_size = args['batch_size'],\n","    device = args['device'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AyeTcfinOhnb","colab_type":"text"},"source":["### Definição da Arquitetura"]},{"cell_type":"code","metadata":{"id":"cmQVTTVWOa9P","colab_type":"code","colab":{}},"source":["class BiLstmTagger(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n","               num_layers, batch_size, embedding_weigths, pad_idx, \n","               unk_idx):\n","    super(BiLstmTagger, self).__init__()\n","    \n","    self.num_layers = num_layers\n","    self.batch_size = batch_size\n","    self.hidden_dim = hidden_dim\n","\n","    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","    self.init_embedding(embedding_weigths, pad_idx, unk_idx, embedding_dim)\n","\n","    self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True)\n","    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","    \n","    self.dropout = nn.Dropout(0.25)\n","    \n","  def init_embedding(self, weights, pad_idx, unk_idx, embedding_size):\n","    self.embed.weight.data.copy_(weights)\n","    self.embed.weight.data[unk_idx] = torch.zeros(embedding_size)\n","    self.embed.weight.data[pad_idx] = torch.zeros(embedding_size)\n","    self.embed.weight.requires_grad=False ## Freeze no update dos embedding pre treinados ate que a rede tenha aprendido\n","\n","  def forward(self, text, lengths):\n","    ## Inicializa automaticamente os estados internos com zeros\n","    \n","    ## Empacote a sequência antes de alimentar a unidade recorrente\n","    embedding = self.embed(text)\n","    packed_embedding = nn.utils.rnn.pack_padded_sequence(embedding, lengths)\n","    \n","    ## Forward recorrente\n","    packed_output, (hn, cn) = self.bilstm(packed_embedding)\n","    unpacked_output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","    ## Linear layer\n","    out = self.fc(self.dropout(unpacked_output))\n","\n","    ## Cross entropy loss already inclues softmax at the end so there is no need for that\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YLdsWK6O0_g","colab_type":"text"},"source":["### Funções Auxiliares"]},{"cell_type":"code","metadata":{"id":"rKTt9DZnOmos","colab_type":"code","colab":{}},"source":["def initialize_weights(*models):\n","  for model in models:\n","    for k, module in enumerate(model.modules()):\n","      if isinstance(module, nn.LSTM):\n","        for name, param in module.named_parameters():\n","          if 'weight' in name:\n","            nn.init.xavier_normal_(param.data)\n","          elif 'bias' in name:\n","            nn.init.constant_(param.data, 0)\n","      elif isinstance(module, nn.Linear):\n","        nn.init.xavier_normal_(module.weight)\n","        if module.bias is not None:\n","            module.bias.data.zero_()\n","\n","\n","def count_parameters(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def categorical_accuracy(preds, y, tag_pad_idx):\n","  \"\"\"\n","  Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","  \"\"\"\n","  max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n","  non_pad_elements = (y != tag_pad_idx).nonzero()\n","  correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n","  return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])\n","\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIcxcmgHOrcK","colab_type":"code","outputId":"c654b402-3eaa-40d7-dd92-0b8f012c39a5","executionInfo":{"status":"ok","timestamp":1574562297842,"user_tz":180,"elapsed":38190,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["output_dim = len(LABEL.vocab)\n","vocab_size     = len(TEXT.vocab)\n","\n","pretrained_embeddings = TEXT.vocab.vectors\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","net = BiLstmTagger(vocab_size, args['embedding_dim'], args['hidden_dim'],\n","                  output_dim, args['num_layers'], args['batch_size'],\n","                  pretrained_embeddings, PAD_IDX,\n","                  UNK_IDX).to(args['device'])\n","\n","initialize_weights(net)\n","print(net)\n","\n","print(f'The model has {count_parameters(net):,} trainable parameters')"],"execution_count":142,"outputs":[{"output_type":"stream","text":["BiLstmTagger(\n","  (embed): Embedding(23245, 100, padding_idx=1)\n","  (bilstm): LSTM(100, 100, num_layers=2, bidirectional=True)\n","  (fc): Linear(in_features=200, out_features=28, bias=True)\n","  (dropout): Dropout(p=0.25, inplace=False)\n",")\n","The model has 408,828 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VQQKsgWKPFkz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":324},"outputId":"49711783-be3e-479a-9d15-0a1c7df11a88","executionInfo":{"status":"error","timestamp":1574562297863,"user_tz":180,"elapsed":38198,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}}},"source":["optimizer = optim.Adam(net.parameters(),\n","                       lr=args['lr'],\n","                       betas=(args['momentum'], 0.999),\n","                       weight_decay=args['weight_decay'])\n","\n","for state in optimizer.state.values():\n","  for k, v in state.items():\n","    if isinstance(v, torch.Tensor):\n","        state[k] = v.to(args['device'])\n","\n","TAG_PAD_IDX = LABEL.vocab.stoi[LABEL.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX).to(args['device'])\n","\n","current_time = str(datetime.datetime.now().timestamp())"],"execution_count":143,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-143-f82ab8a2b816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrain_log_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logs/tensorboard/train/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_summary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.summary' has no attribute 'create_file_writer'"]}]},{"cell_type":"markdown","metadata":{"id":"DboCZWCbPyC3","colab_type":"text"},"source":["### Treinando Modelo e Avaliando Acurácia no Teste"]},{"cell_type":"code","metadata":{"id":"rHtdfKsnPPd_","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion, tag_pad_idx):\n","    \n","  epoch_loss = 0\n","  epoch_acc = 0\n","  \n","  model.train()\n","  \n","  for batch in iterator:\n","      \n","    \n","\n","    text, lengths = batch.text\n","    tags = batch.labels\n","    \n","    optimizer.zero_grad()\n","      \n","    predictions = model(text, lengths)\n","    \n","    predictions = predictions.view(-1, predictions.shape[-1])\n","    tags = tags.view(-1)\n","\n","    loss = criterion(predictions, tags)\n","            \n","    acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","    \n","    loss.backward()\n","\n","    if args['gradient_clipping']:\n","      torch.nn.utils.clip_grad_norm_(net.parameters(), args['clip_norm'])\n","    \n","    optimizer.step()\n","    \n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","      \n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion, tag_pad_idx):\n","    \n","  epoch_loss = 0\n","  epoch_acc = 0\n","  \n","  model.eval()\n","  \n","  with torch.no_grad():\n","  \n","    for batch in iterator:\n","\n","      text, lengths = batch.text\n","      tags = batch.labels\n","      \n","      predictions = model(text, lengths)\n","      \n","      predictions = predictions.view(-1, predictions.shape[-1])\n","      tags = tags.view(-1)\n","      \n","      loss = criterion(predictions, tags)\n","      \n","      acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","      \n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","best_valid_loss = float('inf')\n","\n","if args['train_model']:\n","  for epoch in range(args['epoch_num']):\n","\n","    if epoch == args['epoch_finetune']: ## Unfreeze embedding layer finetunning\n","      net.embed.weight.requires_grad=True\n","      net.dropout.p = 0.4\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(net, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n","    valid_loss, valid_acc = evaluate(net, valid_iterator, criterion, TAG_PAD_IDX)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(net.state_dict(), 'model.pt')\n","\n","    '''with train_summary_writer.as_default():\n","      tf.summary.scalar('train-loss', train_loss, step=epoch)\n","      tf.summary.scalar('train-acc', train_acc, step=epoch)\n","      tf.summary.scalar('validation-loss', valid_loss, step=epoch)\n","      tf.summary.scalar('validation-acc', valid_acc, step=epoch)'''\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","  test_loss, test_acc = evaluate(net, test_iterator, criterion, TAG_PAD_IDX)\n","\n","  print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')\n","\n","else: ## NOT WORKING PROPERLY\n","  net.load_state_dict(torch.load('model.pt'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6XVSJKHN1qq","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs/tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yoKdktxZ9AyD","colab_type":"code","colab":{}},"source":["net.eval()\n","  \n","errors_dict = {i : 0 for i, _ in enumerate(list(LABEL.vocab.stoi))}\n","all_labels = []\n","\n","with torch.no_grad():\n","\n","  for batch in test_iterator:\n","\n","    text, lengths = batch.text\n","    tags = batch.labels\n","    \n","    predictions = net(text, lengths)\n","    \n","    predictions = predictions.view(-1, predictions.shape[-1])\n","    preds = predictions.argmax(dim = 1, keepdim = True).cpu().numpy()\n","\n","    tags = tags.view(-1).cpu().numpy().reshape(preds.shape[0], 1)\n","    tags_list = tags.tolist()\n","\n","    flat_tags = [item for sublist in tags_list for item in sublist]\n","    all_labels.append(flat_tags)\n","\n","    errors =  (preds!=tags)\n","    errors_index, _ = np.where(errors)\n","\n","    for error_idx in preds[errors_index]:\n","      errors_dict[error_idx[0]] = errors_dict[error_idx[0]] + 1\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7244rsWDJIl","colab_type":"code","colab":{}},"source":["named_errors_dict = {LABEL.vocab.itos[i]: value for i, value in errors_dict.items()}\n","x = named_errors_dict\n","del x['<pad>']\n","del x['<unk>']\n","sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)\n","\n","names = [name for name, _ in sorted_x]\n","values = [value for _, value in sorted_x]\n","sorted_x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vObSTHl_UpZm","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(16,9))\n","\n","x = list(range(len(values)))\n","\n","plt.bar(x, height=values)\n","plt.xticks(fontsize=14, rotation=90)\n","plt.xticks(x, names)\n","plt.yticks(list(np.arange(0, 8500, 500)))\n","plt.ylabel('Contagem de classificações erradas')\n","plt.xlabel('Classe')\n","plt.title('Contagem de classificações erradas por classe no conjunto de teste')\n","plt.savefig('contagem_class.png', dpi=300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1o12FgHcqqc","colab_type":"code","colab":{}},"source":["LABEL.build_vocab(test_data)\n","\n","perc_class = []\n","\n","for i, (tag, count, percent) in enumerate(tag_percentage(LABEL.vocab.freqs.most_common())):\n","  print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")\n","  perc_class.append(values[i]/count)\n","\n","accuracy_per_class = np.around(1-np.array(perc_class), 2)\n","\n","plt.figure(figsize=(16,9))\n","\n","x = list(range(len(values)))\n","\n","plt.bar(x, height=accuracy_per_class)\n","plt.xticks(fontsize=14, rotation=90)\n","plt.xticks(x, names)\n","plt.yticks(list(np.arange(0, 1.05, 0.05)))\n","plt.ylabel('Acurácia')\n","plt.xlabel('Classe')\n","plt.title(\"Acurácia por classe no conjunto de teste\")\n","plt.savefig('acc_class.png', dpi=300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1USbX89j6YcQ","colab_type":"code","colab":{}},"source":["model = KeyedVectors.load_word2vec_format('skip_s100.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtjzDnCs6htf","colab_type":"code","colab":{}},"source":["tdata = pd.read_csv('test_data.csv', index_col=\"Unnamed: 0\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSXmRa9I62uc","colab_type":"code","colab":{}},"source":["nembedd_dict = {name: 0 for name in LABEL.vocab.itos if name not in '<pad><unk>'}\n","test_labels_dict = {name: 0 for name in LABEL.vocab.itos if name not in '<pad><unk>'}\n","\n","\n","for idx, line in enumerate(tdata['text']):\n","  words = line.split()\n","  labels = tdata['labels'][idx].split()\n","\n","  for i, word in enumerate(words):\n","    label = labels[i]\n","    test_labels_dict[label] = test_labels_dict[label] + 1\n","    try:\n","      model.get_vector(word)\n","    except KeyError:\n","      nembedd_dict[label] = nembedd_dict[label] + 1 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N67dCa369aXB","colab_type":"code","colab":{}},"source":["embedd_stats = []\n","for name in names:\n","  embedd_stats.append(nembedd_dict[name]/test_labels_dict[name])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7jKHd8Xu9EB5","colab_type":"code","colab":{}},"source":["fault_per_class = np.around(np.array(embedd_stats)*100, 3)\n","\n","plt.figure(figsize=(16,9))\n","\n","x = list(range(len(fault_per_class)))\n","\n","plt.bar(x, height=fault_per_class)\n","plt.xticks(fontsize=14, rotation=90)\n","plt.xticks(x, names)\n","plt.yticks(list(np.arange(0, 70, 5)))\n","plt.ylabel('Porcentagem de amostras sem embedding no modelo pré-treinado')\n","plt.xlabel('Classe')\n","plt.title(\"Porcentagem de amostras sem embedding no modelo pré-treinado por classe\")\n","plt.savefig('perc_embedd.png', dpi=300)"],"execution_count":0,"outputs":[]}]}